\documentclass[11pt]{amsart} 
\usepackage{amsmath, amssymb, amsthm}
\usepackage[marginratio = 1:1,
     height = 601pt, 
     width = 435pt,
     tmargin = 95pt]{geometry}
\usepackage{listings}
 \lstset{language=R, 
     showspaces=false,
     showtabs=false,
     showstringspaces=false,
     basicstyle=\ttfamily
     \small,
     backgroundcolor=\color{mygray},
     frame=single,
     framerule=0.0pt}
\usepackage{graphicx}
\usepackage{xcolor}
\newcommand{\collineB}{0,0.3,0.8} 
\definecolor{mycolB}{rgb}{\collineB}
\definecolor{mygray}{rgb}{1,1,1}
\usepackage[colorlinks, 
     linkcolor=mycolB, 
     citecolor=blue,
     urlcolor=magenta]{hyperref}
\usepackage{verbatim}
% $$$:
\renewcommand{\rmdefault}{ptm} % times
\renewcommand*\ttdefault{txtt} % typewriter is txtt
\renewcommand*\sfdefault{phv}  % helvetica
\usepackage[subscriptcorrection]{mtpro}
\usepackage[mtphrb]{mtpams}
\usepackage[mtpcal, mtpfrak]{mtpb}
%\usepackage[scaled=1.1]{rsfso}
%
%\DeclareMathAlphabet{\txcal}{U}{tx-cal}{m}{n}
\DeclareMathAlphabet{\rsocl}{U}{rsfso}{m}{n}
%\DeclareFontEncoding{FMS}{}{}
%  \DeclareFontSubstitution{FMS}{futm}{m}{n}
%\DeclareMathAlphabet{\foucal}{FMS}{futm}{m}{n}
%
\newcommand{\wP}{P^\ast}
\newcommand{\wE}{E^\ast}
\newcommand{\wZ}{Z^\ast}
\newcommand{\wt}{\theta^\ast}
\newcommand{\wk}{\psi^\ast}
\newcommand{\whp}{\widehat \pi}
\newcommand{\wAe}{A_e^\ast}
\newcommand{\wAp}{A_p^\ast}
\newcommand{\wBe}{B_e^\ast}
\newcommand{\wBp}{B_p^\ast}
%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\setcounter{tocdepth}{3} % to get subsubsections in toc
\let\oldtocsection=\tocsection
\let\oldtocsubsection=\tocsubsection
\let\oldtocsubsubsection=\tocsubsubsection
\renewcommand{\tocsection}[2]{\hspace{0em}\oldtocsection{#1}{#2}}
\renewcommand{\tocsubsection}[2]{\hspace{.9em}\oldtocsubsection{#1}{#2}}
\renewcommand{\tocsubsubsection}[2]{\hspace{.5em}\oldtocsubsubsection{#1}{#2}}

%\makeatletter
%\def\@tocline#1#2#3#4#5#6#7{\relax
%  \ifnum #1>\c@tocdepth % then omit
%  \else
%    \par \addpenalty\@secpenalty\addvspace{#2}%
%    \begingroup \hyphenpenalty\@M
%    \@ifempty{#4}{%
%      \@tempdima\csname r@tocindent\psimber#1\endcsname\relax
%    }{%
%      \@tempdima#4\relax
%    }%
%    \parindent\z@ \leftskip#3\relax \advance\leftskip\@tempdima\relax
%    \rightskip\@pnumwidth plus4em \parfillskip-\@pnumwidth
%    #5\leavevmode\hskip-\@tempdima
%      \ifcase #1
%       \or\or \hskip .5em \or \hskip 2em \else \hskip 3em \fi%
%      #6\nobreak\relax
%    \dotfill\hbox to\@pnumwidth{\@tocpagenum{#7}}\par
%    \nobreak
%    \endgroup
%  \fi}
%\makeatother

\makeatletter
\def\@fnsymbol#1{%
  \ensuremath{%
    \ifcase#1% 0
    \or % 1
      \dagger%   
    \or % 2
      *
    \or % 3  
      \ddagger
    \or % 4   
      \mathsection
    \or % 5
      \mathparagraph
    \else % >= 6
      \@ctrerr  
    \fi
  }%   
}   
\makeatother

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}
\title[\textup{R. A. Rosales, R. D. Drummond, 
       R. Valieris, E. Dias-Neto, I. T. da Silva}]{} 
\noindent{\parbox{\linewidth}{\footnotesize %
    {\footnotesize\textsl{Submitted to Bioinformatics }}
    {\footnotesize\textrm{ 29/02/16 - v 0.5}}
  }
 }\\[1em]
 \begin{center}
   {\large Supplementary material to:}\\[1em]
   {\Large\sc An empirical Bayesian approach}\\[0.3em] 
   {\Large\sc to Mutational Signature Discovery}\\[2em]
   {\large 
     Rafael A. Rosales\footnote{R. A. Rosales and R. D. Drummond are
       both to be considered as First Author}, 
     Rodrigo D. Drummond$^\dagger$,
     Renan Valieris, Emmanuel Dias-Neto  and
     Israel T. da Silva\footnote{Corresponding
       author}$^,$\footnote{Partially supported by FAPESP grant */*}}
 \end{center}


\maketitle

%  {Rafael A. Rosales}\\
%  {\footnotesize\it Deparatmento de Computa\c{c}\~ao e Matem\'atica, 
%   Universidade de S\~ao Paulo}\\ 
%  {\footnotesize\it Av. Bandeirantes, 3900, Ribeir\~ao Preto, 
%  14049-901, SP, Brazil}\\{\footnotesize\verb+rrosales@usp.br+}\\[1em]
%%
%  {Rodrigo Drummond, Renan Valieris}\\
%  {\footnotesize\it Laboratory of Bioinformatics and Computational 
%  Biology, CIPE/A.C. Camargo Cancer Center\\ S\~ao Paulo 
%  01509-010,  Brazil}\\
%  {\footnotesize\verb+rddrummond@gmail.com+,    
%     \verb+rvalieris@gmail.com+}\\[1em] 
%%
%  {Israel T. da Silva}\\
%  {\footnotesize\it Laboratory of Bioinformatics and Computational 
%   Biology, CIPE/A.C. Camargo Cancer Center\\ S\~ao Paulo 
%   01509-010, Brazil}\\
%  {\footnotesize\it and}\\
%  {\footnotesize\it Laboratory of Molecular Immunology, 
%  The Rockefeller University}\\
%  {\footnotesize\it 1230 York Avenue, New York, NY 10065}\\
%  {\footnotesize\verb+itojal@gmail.com+}\\[1.5em]
%\end{center} 

\tableofcontents 

\section{Notation and preliminary Results}
Let $M$ be matrix of mutation counts of dimension $K\times G$ and let
$m$ denote a particular sample for $M$. For the factorization $M=PE$,
the observations $(M)_{ij}$ are independent and Poisson distributed
random variables with rates $(PE)_{ij}(W)_{ij}$, with $W$ as the
$K\times G$ opportunity matrix. The factors $P$, $E$, identified as
the model parameters are denoted as $\theta$. The same as 
\cite{FICMV}, we regard $W$ as fixed and known and set $w_{ij} = 1$
for all $i$ and $j$ if $W$ is not determined out from the data. The
likelihood for this model, i.e. the function $\rsocl L: \Theta \to 
\mathbb R$ defined by $\theta \mapsto p(M=m|\theta)$, is given by 
\begin{equation}
  \label{eqn:PoisLik}
   \rsocl L(\theta; m) 
   =
    \prod_{i=1}^K \prod_{j=1}^G e^{-w_{ij}\sum_{n=1}^{N}p_{in}e_{nj}}
    \Big(w_{ij}\sum_{n=1}^{N}p_{in}e_{nj}\Big)^{m_{ij}}
    \frac{1}{m_{ij}!}.\tag{$s_1$}
\end{equation}

Posterior inferences about the factors $P$ and $E$ and the
factorization rank $N$ require the joint distribution for the
observations $M$ and the latent variables $Z$. This is also known as
the complete data likelihood function when interpreted as a function
of  $\theta$ for given $Z=z$ and $M=m$. Following the condition 
(1) in the main text and the independence between
the components of $Z$, this distribution equals
\begin{equation}
   \label{eqn:jointdata}
 \begin{aligned}
    p(Z = z, M =&\ m \mid P, E) \notag\\
  &= 
    \prod_{n=1}^N\prod_{i=1}^K\prod_{j=1}^G p\Big(Z_{inj} = z_{inj},
    M_{ij} = m_{ij}, \mathbf{1}_{\big\{M_{ij} = \sum_{n=1}^N
      Z_{inj}\big\}} \Big| P, E\Big)  \notag\\ 
  &=
    \prod_{n=1}^N\prod_{i=1}^K\prod_{j=1}^G e^{-p_{in}e_{nj}w_{ij}}
    (p_{in}e_{nj}w_{ij})^{z_{inj}} \frac{1}{z_{inj}!}
    \mathbf{1}_{\big\{m_{ij} = \sum_{n=1}^N z_{inj}\big\}}.
 \end{aligned}
 \tag{$s_2$}
\end{equation}
The symbol $\mathbf{1}$ denotes the indicator function for the  
event $\big\{M_{ij} = \sum_n Z_{inj}\big\}$, with value 1 if $M_{ij} =  
\sum_n  Z_{inj}$ and 0 otherwise. Marginalization of
(\ref{eqn:jointdata}) with respect to $Z$ gives  (\ref{eqn:PoisLik}).


\section{Gibbs sampler}
\label{sec:Gibbs}
The construction of the Gibbs sampler relies on the determination of
the full conditional distributions for all unknowns in the
hierarchical model, that is, the distribution of each component of
$(Z, \theta, \psi)$ conditioned on all other components of this vector,
the data $M$ and the hyperprior parameters $\eta$,
\[
   Z \sim \pi(Z|\theta, \psi, M, \eta), \quad
   \theta \sim \pi(\theta|Z, \psi, M, \eta) \quad
  \text{and}\quad
  \psi \sim \pi(\psi|Z, \theta, M, \eta).
\]
The conditional distributions for $\theta$ and $\psi$ are obtained by
straightforward computations following the likelihood in
(\ref{eqn:PoisLik}) and the hierarchical model defined througout the
main text. Only the conditional for $Z$, which also depends on the
complete data likelihood in (\ref{eqn:jointdata}), deserves special
attention. The conditional distribution for $Z$ is derived in
Lemma~\ref{lem:Full_for_Z}.


To begin with, by \emph{a priori} indepence between $P$ and $E$, for
the conditional for $\theta$ one has that 
\[
   \pi(\theta|Z, \psi, M, \eta) 
   = 
   \prod_{i=1}^K\prod_{n=1}^N\prod_{j=1}^G
    \pi(p_{in}|Z, \psi, M, \eta)
    \pi(e_{nj}|Z, \psi, M, \eta).
\]
Direct computations show that, for any $1 \leq i\leq K$ and $1\leq
n\leq  N$, $p_{in}$ has the following Gamma density
\begin{equation}
  \label{eqn:Full_for_P}
  p_{in} 
       \sim 
     \text{Gamma}\Big(p_{in}\,\Big|\, \alpha_{in}^p + 1 +
     \sum_{j=1}^G z_{inj}, \beta_{in}^p +
     \sum_{j=1}^G e_{nj}w_{ij}\Big).\tag{$s_2$}
\end{equation}
Similarly, the full conditional for $e_{nj}$, for any $1\leq n\leq 
N$, $1\leq j\leq G$, follows the Gamma density  
\begin{equation}
  \label{eqn:Full_for_E}
  e_{nj} 
     \sim 
   \text{Gamma}\Big(e_{nj}\,\Big|\, \alpha_{nj}^e + 1 +
   \sum_{i=1}^K z_{inj}, \beta_{nj}^e +
   \sum_{i=1}^K p_{in}w_{ij}\Big).\tag{$s_3$}
\end{equation}


Due to \emph{a priori} independence between the
components of the matrices $A_e$, $B_e$, $A_p$ and $B_p$, the full
conditional distribution for the hyperparameters $\psi$ equals the
product
\[
   \prod_{i=1}^K\prod_{n=1}^N
     p(\alpha_{in}^p| \theta, Z, M, \eta)
     p(\beta_{in}^p|\theta, Z, M, \eta)
   \prod_{n=1}^N\prod_{j=1}^G
     p(\alpha_{nj}^e|\theta, Z, M, \eta)
     p(\beta_{nj}^e|\theta, Z, M, \eta).
\]
The update for $\psi$ runs therefore through the update of each
element of these matrices. Up to proportionality, each entry of the
$B_p$ matrix, $\beta_{in}^p$,  has the full conditional density
\begin{equation}
 \label{eqn:Full_for_Bp}
 \beta_{in}^p
   \propto
   (\beta_{in}^p)^{a_p + \alpha_{in}^p} 
   \exp\Big(-(p_{in}+b_p)\beta_{in}^p\Big),\tag{$s_4$}
\end{equation}
for $\beta_{in}^p > 0$. This corresponds to a Gamma density with shape
$\alpha_{in}^p + 1 + a_p$ and rate $p_{in} + b_p$.  Likewise, for each
entry of the $B_e$ matrix, the full conditional has density 
\begin{equation}
 \label{eqn:Full_for_Be}
 \beta_{nj}^e
   \propto
  (\beta_{nj}^e)^{a_e + \alpha_{nj}^e} 
  \exp\Big(-(e_{nj}+b_e)\beta_{nj}^e\Big),\tag{$s_5$}
\end{equation}
which is a  Gamma density for $\beta_{nj}^e > 0$, with shape
$\alpha_{nj}^e + 1+ a_e$ and rate $e_{nj}+b_e$. 


The exponential prior for the elements of $A_p$ leads, up to
proportionality, to the following the full conditional density
for\footnote{\textcolor{red}{RD: I know that we already went though
    this, but  is it safe to take the constant $\lambda_p(\beta_{in}^p
    + \delta_p)$ away from the density for $\alpha_{in}^p$ --because
    it  cancels when one divides by the integral that will define the
    normalising constant?}}
$\alpha_{ij}^p$,
\begin{equation}
 \label{eqn:Full_for_Ap}
  \alpha_{in}^p 
  \sim 
  \lambda_p\frac{\beta_{in}^p}{\Gamma(\alpha_{in}^p + 1)} 
   \Big[\beta_{in}^pp_{in}
   e^{-\lambda_p}\Big]^{\alpha_{in}^p}, \tag{$s_6$}
   \quad  \alpha_{in}^p > 0.
\end{equation}
Similarly, up to proportionality, for the elements of $A_e$ we
have\footnote{\textcolor{red}{RD: same here, should we take away the
    constant 
  $\lambda_e(\beta_{nj}^e + \delta_e)$ in the density for
  $\alpha_{nj}^e$?}}
\begin{equation}
 \label{eqn:Full_for_Ae}
  \alpha_{nj}^e 
  \sim
  \lambda_e\frac{\beta_{nj}^e}{\Gamma(\alpha_{nj}^e + 1)} 
   \Big[\beta_{nj}^e e_{nj}
   e^{-\lambda_e}\Big]^{\alpha_{nj}^e}, 
   \quad  \alpha_{nj}^e > 0. \tag{$s_7$}
\end{equation}
The full conditional distributions for the hyperparameters $A_p$ and
$A_e$ are not from a standard family. Samples from these distributions
are obtained by considering Metropolis-Hastings steps. The
implementation of these is described in Section~\ref{sec:MHsteps}.

The full conditional distribution for the latent variables, $Z$, is a
product of multinomial distributions. This result is precisely stated
by the following Lemma. 

\begin{lemma}\label{lem:Full_for_Z} Conditionally on  $M= m$, the full
  conditional distribution for the latent variables $Z$ is a  product
  of multinomial laws 
\[
   p(Z = z\,|\, \theta, \psi, M=m, \eta)
   = 
   \prod_{i=1}^K\prod_{j=1}^G {m_{ij} \choose z_{i1j}, \ldots,
     z_{iNj}} 
   \prod_{n=1}^N \phi_{inj}^{z_{inj}},
\]
with $\phi_{inj} = p_{in} e_{nj}/\sum_{r=1}^N p_{ir}e_{rj}$.
\end{lemma}

\begin{proof} 
Conditionally on $M = m$, $P$ and $E$, the distribution of $Z$ is
independent of $\psi$ and $\eta$, $p(Z\,|\, M$, $P$, $E$, $\psi$,
$\eta) = p(Z\,|\, M$, $P$, $E)$ because by definition our our model,
$Z$ is a set of Poisson random variables with rates determined by $P$
and $E$. The required full conditional is determined by the ratio
$p(Z\,|\,M, P, E) = p(Z, M\,|\, P,E)/p(M\,|\,P, E)$, that is by
considering the ratio of (\ref{eqn:jointdata}) by
(\ref{eqn:PoisLik}). Taking logarithms leads to
\begin{align*}
    \ln p(Z|M, P, E) 
  =&
    \ln p(Z, M|P, E) - \rsocl L(\theta; m) \\
  =&
    \sum_{i=1}^K \sum_{j=1}^G \sum_{n=1}^N z_{inj}
     \ln(p_{in}e_{nj}w_{ij}) - 
     p_{in}e_{nj}w_{ij} - \ln(z_{inj}!) +\ln \mathbf{1}_{\big\{M_{ij} =
     \sum_{n=1}^N Z_{inj}\big\}} \\ 
    & -\sum_{i=1}^K \sum_{j=1}^G m_{ij}\ln \sum_{u=1}^N
      p_{iu}e_{uj}w_{ij} - 
      \sum_{u=1}^N p_{iu}e_{uj}w_{ij} + \ln m_{ij}!.
\end{align*}
Direct simplifications obtained by  setting $m_{ij}$ equal to
$\sum_{n=1}^N  z_{inj}$ give
\begin{align*}
    \ln p(Z|M, P, E) 
  =&
    \sum_{i=1}^K \sum_{j=1}^G\Big\{
       \sum_{n=1}^N \Big(
           z_{inj} \ln\frac{p_{in}e_{nj}}{\sum_{u=1}^N
           p_{iu}e_{uj}} - \ln z_{inj}!
       \Big) + 
       \ln \mathbf{1}_{\big\{M_{ij} = \sum_{n=1}^N Z_{inj}
     \big\}} \\
     &+ \ln \big(\sum_{u=1}^N z_{iuj}\big)!
   \Big\}.
\end{align*}
Considering exponentiation to revert to the original scale shows that
the full conditional for $Z$ is fact a product of multinomial
distributions as required,
\begin{align}
       p(Z = z\mid M = m, P, E) 
     &= 
       \prod_{i=1}^K \prod_{j=1}^G
       p(Z_{i1j} = z_{i1j}, \ldots, Z_{iNj} =
       z_{iNj}\mid m_{ij}, P, E) \notag\\ 
     &=
       \prod_{i=1}^K \prod_{j=1}^G
       \frac{m_{ij}!}{z_{i1j}!\cdots z_{iNj}!}
        \phi_{i1j}^{z_{i1j}} \cdots
       \phi_{iNj}^{z_{iNj}}. \notag\qedhere
\end{align}
\end{proof}


\subsection{Metropolis-within-Gibbs steps}\label{sec:MHsteps}
The full conditional distributions for the entries in both $A_p$ and
$A_e$ do not have a standard form. These are explicitly shown in
(\ref{eqn:Full_for_Ap}) and (\ref{eqn:Full_for_Ae}). Draws from these
distributions, necessary to define the Gibbs sampler,  are obtained by
using Metropolis-Hastings steps.  Let $x > 0$ be the current value
for any given entry of $A_e$ (or $A_p$). A new candidate value $y$ for
this variable is generated from a Gamma proposal density, $g(\cdot|
x)$ with shape $(x/\sigma)^2$ and rate $x/\sigma^2$. The mean of this
proposal is thus set to equal $x$ and its variance to $\sigma^2$. The 
results  described in the main text where obtained by
\textcolor{red}{choosing 
  $\sigma^2 = *$}\footnote{\textcolor{red}{RD: please put the value
    here!}}. Let $\ln \alpha$ be defined as
\[
  \ln \alpha 
 =
  \ln p(y) - \ln p(x) + \ln g(x|y) - \ln g(y|x)
\]
with $p(x)$ as the density in (\ref{eqn:Full_for_Ap}) (or in  
(\ref{eqn:Full_for_Ae})). The value $y$ is accepted as a sample
with probability $\rho = \min\{1, e^{\alpha}\}$, that is, if $x'$ denotes
the new sampled value, then
\[
   x'
    =
  \begin{cases}
    x, & \text{if } U > \rho,\\
    y, & \text{if } U \leq \rho.
  \end{cases}
\]
for $U$ an uniformly distributed random variable on $[0, 1]$.

\section{MCMC EM}
\subsection{Maximisation step}
This section describes the maximisation step in the EM algorithm
necessary to derive the estimator $\hat\eta$, namely
\begin{equation}
   \label{eqn:etaMAX}
    \underset{\eta\,\in\,\Lambda}{\text{arg max}}\,
    \frac{1}{R}\sum_{r=1}^R \ln \rsocl L\big(\eta; m, Z^{(r)},
    \theta^{(r)}, \psi^{(r)}\big). \tag{$s_9$} 
\end{equation}
As described in the main text, the solution to (\ref{eqn:etaMAX}) is
used to define the sequence $\hat\eta^{(u)}$, $u \geq 1$. The
components of the latter are denoted throughout as $\hat b_e^{(u)}$,
$\hat a_e^{(u)}$ and so on.  To simplify notation, let $\rsocl L(\eta;
m, Z^{(r)}, \theta^{(r)}, \psi^{(r)}) = \rsocl L(\eta)$. The
maximisation of $\frac{1}{R}\sum_{r} \ln \rsocl L(\eta)$ with respect
to $\eta$ is made by solving $\nabla\frac{1}{R}\sum_{r}\ln \rsocl
L(\eta) = 0$ for $\eta$.  To start, observe that $\ln \rsocl L(\eta)$
admits the form
\begin{equation}
 \label{eqn:ell}
 \begin{aligned}
  \ln \rsocl L(\eta)
  =&
  \ln p(M, Z^{(r)} | \theta^{(r)})  + \ln p(P^{(r)}\,|\, A_p^{(r)},
  B_p^{(r)}) + \ln p(E^{(r)}\,|\, A_e^{(r)},  B_e^{(r)})  \\
  &+
  \ln p(A_p^{(r)}\,|\, \eta) + \ln p(B_p^{(r)}\,|\, \eta) +
  \ln p(A_e^{(r)}\,|\, \eta) + \ln p(B_e^{(r)}\,|\, \eta).
 \end{aligned}
 \tag{$s_{10}$}
\end{equation}
The first three terms of are independent of
$\eta$. Further, each of the remaining terms can be optimised
separately because each one depends on a different component of
$\eta$. Following the exponential hyperprior for $A_p$ we have
\[
   \frac{1}{R}\sum_{r=1}^R \ln  p(A_p^{(r)}\,|\ \lambda_p)
  %=
  % \frac{1}{R}\sum_{r=1}^R \sum_{i=1}^K\sum_{n=1}^N
  % \big(\ln(\lambda_p) - \lambda_p (A_p^{(r)})_{in}\big)
  = 
  KN\ln(\lambda_p) - \frac{1}{R}\lambda_p
  \sum_{r=1}^R\sum_{i=1}^K\sum_{n=1}^N \big(A_p^{(r)}\big)_{in}.
\]
Hence, solving for $\lambda_p$ in 
\[
  \frac{\partial}{\partial\lambda_p} \frac{1}{R}\sum_r \ln
  p(A_p^{(r)}\,|\ \lambda_p)) = 0
\] 
gives
\[
  \widehat\lambda_p = \frac{RKN}{\sum_{r=1}^R \sum_{i=1}^K 
    \sum_{n=1}^N \big(A_p^{(r)}\big)_{in}}.
\]
Likewise, by considering the third of the remaining terms in
(\ref{eqn:ell}), that is, $\frac{1}{R} \sum_r 
p(A_e\,|\, \lambda_e)$, gives
\[
  \widehat\lambda_e = \frac{RNG}{\sum_{r=1}^R \sum_{n=1}^N
    \sum_{j=1}^G \big(A_e^{(r)}\big)_{nj}}.
\]
The maximization of the second and the fourth terms in (\ref{eqn:ell})
must be handled numerically. This situation is analogous to the
maximum likelihood estimation of the scale and the rate parameters of
a Gamma density, which has no closed form solution, see \cite{CW}. For
instance, because of a Gamma hyperprior for $B_p$ with parameters
$a_p$ and $b_p$, for the second term we have that
\[
   \frac{1}{R}\sum_{r=1}^R \ln p(B_p^{(r)}\,|\, \eta)
  =
   \frac{1}{R}\sum_{r=1}^R \bigg(\sum_{i}\sum_{n} a_p\ln b_p  
    - \ln\Gamma(a_p) + 
   (a_p-1)\ln\big[\big(B_p^{(r)}\big)_{in}\big] -  
    b_p\big(B_p^{(r)}\big)_{in}\bigg).
\]
Solving for $b_p$ in 
\[
  \frac{\partial}{\partial b_p}
   \frac{1}{R}\sum_r \ln p(B_p^{(r)}\,|\, \eta) = 0
\]
yields
\[
   \hat b_p = \frac{RKN}{\sum_{r=1}^R \sum_{i=1}^K \sum_{n=1}^N 
     \big(B_p^{(r)}\big)}_{in}  a_p.
\]
Similarly, for $b_e$ we have
\[
   \hat b_e = \frac{RNG}{\sum_{r=1}^R \sum_{n=1}^N  \sum_{j=1}^G 
     \big(B_e^{(r)}\big)}_{nj}  a_e.
\]
\textcolor{red}{Rodrigo: still to write the numerical implementation
of how $a_p$ (and hence also $a_e$) are obtained. Maybe you are using
\cite{J}, in any case please explain.}


\subsection{Convergence}
This section justifies the convergence of $\widehat\pi(\theta|M,
\hat\eta)$ as defined by (3) towards $\pi(\theta|M, \eta)$. The
arguments follow closely those in \cite{C01} but they are
adapted to the hierarchical NMF model considered throughout. 
\begin{lemma}\label{lem:technical} Suppose that the following
  conditions hold:
\begin{itemize}
 \item[(i)] $\hat\eta^{(u)} \to \eta$ as $u \to \infty$ almost
   surely with respect to $m$; 
 \item[(ii)] $h_\eta(\eta')$ defined as
\[
  h_{\eta}(\eta') 
   = 
 \int p(\theta|Z, \psi, M, \eta')p(Z, \psi|M, \eta)\
 \textup{d} Z \textup{d} \psi
\]
is continuous in both $\eta$ and $\eta'$;
\item[(iii)] $\widehat h_\eta(\eta')$, defined as
\begin{align*}
  \widehat h_\eta(\eta') 
  = 
 \frac{1}{R}\sum_{r=1}^R \pi(\theta|Z^{(r)}, \psi^{(r)}, M, \eta')
    \quad \text{with}\quad 
   \begin{aligned}
      &\psi^{(r)} \sim \pi(\psi|Z^{(r-1)}, \theta^{(r)}, M, \eta)\\
      &Z^{(r)} \sim \pi(Z| \psi^{(r)}, \theta^{(r)}, M, \eta) 
   \end{aligned}
\end{align*}
 is continuous in $\eta'$ and stochastically equicontinuous in $\eta$,
that is, for any given $\epsilon > 0$ there is $\delta>0$ such that
$|\eta_1 - \eta_2| < \delta$ implies $|\widehat h_{\eta_1}(\eta') -
\widehat h_{\eta_2}(\eta')|$\ \ for all $\eta_1, \eta_2$ except for a
$\pi$-measurable null set.
 \item[(iv)] the Metropolis-within-Gibbs sampler produces an ergodic
   Markov chain. 
\end{itemize}
Then there exists a subsequence $(r_u)$ such that $\lim_{u\to\infty}
r_u  = \infty$, for which
\[
   \big|\widehat h_{\hat\eta_u}(\hat\eta_u) - h_\eta(\eta)\big| \to 0 
  \quad\text{as}\quad 
   u \to \infty
\]
almost surely with respect to the densities * and *.
\end{lemma} 
\begin{proof} The existence of $(r_u)$ ensuring the required
convergence follows from Lemma~A.1 in \cite{C01}.  The rest of the
proof consists in the direct verification of the hypotheses (i)-(iv). 


The consistency of $\hat\eta_u$ follows by the EM argument exposed in
the main text because $(Z^{(r)}$, $\theta^{(r)}$, $\psi^{(r)})$ is 
asymptotically distributed according to $\pi(Z, \theta, \psi|M,
\eta)$. The second assertion follows by the continuity of the full 
conditional densities for $\theta$ with respect to $\eta'$
and the continuity of the hyperprior density for  $\psi$ with respect
to $\eta$. Likewise, the equicontinuity in (iii) follows 
by observing that $\widehat h_\eta$ is function of the prior and the
hyperprior densities, and the later are continuous respectively with
respect to $\psi$ and $\eta$. The ergodicity of the
Metropolis-within-Gibbs sampler considered throughout follows from the
Harris recurrence of the Markov chain $(Z^{(r)}, \theta^{(r)},  \psi^{(r)})$, $r \geqslant 1$ , see Theorem
12 in \cite{RR}. Harris recurrence is established by observing that
\textcolor{red}{... finish this!}.
\end{proof}

Let $\mathcal B = \sigma(\Theta)$ be the Borel sigma-algebra generated  
by the open sets of $\Theta$ and let a.e. stand for `almost everywhere'.

\begin{theorem} Under the conditions in Lemma~\ref{lem:technical},
  the statistic defined in \textup{(3)} satisfies
\[
  %\lim_{R, u \to\infty} 
  \sup_{S \in  \mathcal B} 
     \big| 
         \hat\pi(S|M, \hat\eta) -  \pi(S|M, \eta)
     \big| \to 0 \quad \text{as}\quad R, u \to \infty.
\]
\end{theorem}
\begin{proof}
From Lemma~\ref{lem:technical} it follows that $\widehat h_{\hat\eta_u}$
converges a.e. to $h_\eta$ as $u, R \to \infty$, that is
\[
  \lim_{u, R \to \infty} \widehat\pi(\theta|M, \hat\eta) 
  =
  \pi(\theta|M,\eta)\qquad\text{a.e.}
\]
This together with Scheff\'es Lemma, see for instance Theorem 7 in
\cite{DG}, implies that 
\[
  \lim_{u, R \to \infty} 
  \int \big|\widehat\pi(\theta|M, \hat\eta) - \pi(\theta|M,
  \eta)\big|\ \text{d}m(\theta) 
  =  
  0,
\]
where integration is with respect to Lebesgue measure defined in
$(\Theta, \mathcal B)$. This immediately gives
\begin{align*}
  \int \big|\widehat\pi(\theta &| M, \hat\eta) 
   - 
  \pi(\theta|M, \eta)\big|\ \text{d}m(\theta)                \\
  &=
  2 \sup_{B \in \mathcal B}\Bigg|
      \int_B \widehat\pi(B|M, \hat\eta)\ \text{d}m(\theta) 
      -
      \int_B \pi(B|M, \eta)\ \text{d}m(\theta)
   \Big| \to 0
\end{align*}
as $u, R \to \infty$, see Theorem 1 in \cite{DG}, and concludes
therefore the proof. 
\end{proof}


\section{Simulation study}
Put/describe the results for the `gold standard' data set. Also put
the result of EMu on this. 

\section{Installing and running \texttt{signeR}}
\subsection{Installing \texttt{signeR}}
\texttt{signeR} is available as an R package and it can be installed
from R's prompt by typing
\begin{lstlisting}[]
  install.packages('signeR', dependencies=TRUE)
\end{lstlisting}
This should install also the following dependencies:
\textcolor{red}{....  Now comes the difficult bit: are we distributing
pre-compiled binaries at CRAN, or just source? Note that the first
option needs someone to mantain this over time. In the second case,
the installation relies upon the compilation of C++ code, and hence
depends on the existence of a properly working C compiler. Have to
explain on how to do this in a Mac OS X and Windows.}


\subsection{Running \texttt{signeR}}
Write: A step-by-step example about how \texttt{signeR} would be run
on a real data example, probably on the 21 breast cancer data.
Loading the package \verb+library(signeR)+ and then typing
\verb+eBayesNMF()+ with the following arguments. Explain the data
structure for the input matrices $M$, $W$.


Mention the actual dependencies \cite{Boo}, etc, and the also
R's NMF package.
\begin{lstlisting}[]
  # this is a comment
  Mut<-read.table('21_breast_cancers.mutations.txt',header=FALSE)
  Opp<-read.table('21_breast_cancers.opportunity.txt',header=FALSE)
  Outsig<-signeR(M=Mut, Opport=Opp)  
  Paths(Outsig$SignExp)
  SignPlot(Outsig$SignExp)
  SignBoxPlot(Outsig$SignExp)
  Classify(Outsig$SignExp, labels=c(rep(`GradeII',10),
     rep(`GradeIII',10), NA))
  DiffExp(Outsig$SignExp, labels=c(rep(`GradeII',10),
     rep(`GradeIII',11)))
\end{lstlisting}

%\bibliographystyle: natbib, achemnat, plainnat, abbrv, plain 
\bibliographystyle{alpha}
\bibliography{suppl}


\vspace{1cm}

{\footnotesize
\begin{tabular}{ll}
 \centering
  \parbox[c][4cm][t]{8cm}{
   {\sc Rafael A. Rosales}\\
   {\it 
   Departamento de Computa\c{c}\~ao e Matem\'atica\\
   Universidade de S\~ao Paulo\\
   Av. Bandeirantes, 3900, Ribeir\~ao Preto\\
   S\~ao Paulo 14049-901, Brazil\\}
   E-mail: \verb~rrosales@usp.br~
  }
&
  \parbox[c][4cm][t]{6.6cm}{
   {\sc Rodrigo D. Drummond\\
       Renan Valieris}\\
    {\it 
    Laboratory of Bioinformatics and Computational\\
    Biology, A. C. Camargo Cancer Center\\ 
    S\~ao Paulo 01509-010, Brazil\\}
    E-mails: \parbox[t]{2.5cm}{%
      \verb~rddrummond@gmail.com~\\ 
      \verb~rvalieris@gmail.com~}
 }
\\[-2em]
  \parbox[c][4cm][t]{6.6cm}{
   {\sc Emmanuel Dias-Neto}\\
    {\it 
    Laboratory of Medical Genomics\\ 
    A. C. Camargo Cancer Center\\
    S\~ao Paulo 01509-010, Brazil\\}
    E-mail: \parbox[t]{2.5cm}{%
      \verb~someone@somewhere.com~}
 }
&
  \parbox[c][4cm][t]{6.6cm}{
   {\sc Israel T. da Silva}\\
   {\it 
    Laboratory of Bioinformatics and Computational\\
    Biology, A. C. Camargo Cancer Center\\ 
    S\~ao Paulo 01509-010, Brazil\\}
    and\\
   {\it Laboratory of Molecular Immunology\\
    The Rockefeller University\\
    1230 York Avenue, New York, NY 10065\\}
    E-mail: \verb~itojal@gmail.com~
 }
\end{tabular}
}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
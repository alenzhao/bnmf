\documentclass[11pt]{amsart}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[marginratio = 1:1,
     height = 601pt, 
     width = 435pt,
     tmargin = 95pt]{geometry}
\usepackage{listings}
 \lstset{language=R, 
     showspaces=false,
     showtabs=false,
     showstringspaces=false,
     basicstyle=\ttfamily
     \small,
     backgroundcolor=\color{mygray},
     frame=single,
     framerule=0.0pt}
\usepackage{graphicx}
\usepackage{xcolor}
\newcommand{\collineB}{0,0.3,0.8} 
\definecolor{mycolB}{rgb}{\collineB}
\definecolor{mygray}{rgb}{1,1,1}
\usepackage[colorlinks, 
     linkcolor=mycolB, 
     citecolor=blue,
     urlcolor=magenta]{hyperref}
\usepackage{verbatim}
% $$$:
\renewcommand{\rmdefault}{ptm} % times
\renewcommand*\ttdefault{txtt} % typewriter is txtt
\renewcommand*\sfdefault{phv}  % helvetica
\usepackage[subscriptcorrection]{mtpro}
\usepackage[mtphrb]{mtpams}
\usepackage[mtpcal, mtpfrak]{mtpb}
%
\DeclareMathAlphabet{\txcal}{U}{tx-cal}{m}{n}
\DeclareFontEncoding{FMS}{}{}
  \DeclareFontSubstitution{FMS}{futm}{m}{n}
\DeclareMathAlphabet{\foucal}{FMS}{futm}{m}{n}
%
\newcommand{\wP}{P^\ast}
\newcommand{\wE}{E^\ast}
\newcommand{\wZ}{Z^\ast}
\newcommand{\wt}{\theta^\ast}
\newcommand{\wk}{\psi^\ast}
\newcommand{\whp}{\widehat \pi}
\newcommand{\wAe}{A_e^\ast}
\newcommand{\wAp}{A_p^\ast}
\newcommand{\wBe}{B_e^\ast}
\newcommand{\wBp}{B_p^\ast}
\newcommand{\tM}{\foucal M}
%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\setcounter{tocdepth}{3} % to get subsubsections in toc
\let\oldtocsection=\tocsection
\let\oldtocsubsection=\tocsubsection
\let\oldtocsubsubsection=\tocsubsubsection
\renewcommand{\tocsection}[2]{\hspace{0em}\oldtocsection{#1}{#2}}
\renewcommand{\tocsubsection}[2]{\hspace{.9em}\oldtocsubsection{#1}{#2}}
\renewcommand{\tocsubsubsection}[2]{\hspace{.5em}\oldtocsubsubsection{#1}{#2}}

%\makeatletter
%\def\@tocline#1#2#3#4#5#6#7{\relax
%  \ifnum #1>\c@tocdepth % then omit
%  \else
%    \par \addpenalty\@secpenalty\addvspace{#2}%
%    \begingroup \hyphenpenalty\@M
%    \@ifempty{#4}{%
%      \@tempdima\csname r@tocindent\psimber#1\endcsname\relax
%    }{%
%      \@tempdima#4\relax
%    }%
%    \parindent\z@ \leftskip#3\relax \advance\leftskip\@tempdima\relax
%    \rightskip\@pnumwidth plus4em \parfillskip-\@pnumwidth
%    #5\leavevmode\hskip-\@tempdima
%      \ifcase #1
%       \or\or \hskip .5em \or \hskip 2em \else \hskip 3em \fi%
%      #6\nobreak\relax
%    \dotfill\hbox to\@pnumwidth{\@tocpagenum{#7}}\par
%    \nobreak
%    \endgroup
%  \fi}
%\makeatother

\makeatletter
\def\@fnsymbol#1{%
  \ensuremath{%
    \ifcase#1% 0
    \or % 1
      \dagger%   
    \or % 2
      *
    \or % 3  
      \ddagger
    \or % 4   
      \mathsection
    \or % 5
      \mathparagraph
    \else % >= 6
      \@ctrerr  
    \fi
  }%   
}   
\makeatother

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}
\title[R. A. Rosales, R. D. Drummond, R. Valieris, I. T. da Silva]{}
\noindent{\parbox{\linewidth}{\footnotesize %
    {\footnotesize\textsl{Submitted to Bioinformatics }}
    {\footnotesize\textrm{ 29/02/16 - v 0.5}}
  }
 }\\[1em]
 \begin{center}
   {\Large\bf Supplementary material to:\\[0.4em]
   An empirical Bayesian approach\\[0.3em] 
   to Mutational Signature Discovery\\[2em]}
   {\large 
     Rafael A. Rosales\footnote{R. A. Rosales and R. D. Drummond are
       both to be considered as First Author}, 
     Rodrigo D. Drummond$^\dagger$,
     Renan Valieris and 
     Israel T. da Silva\footnote{Corresponding author}$^,$\footnote{Partially supported by FAPESP grant */*}}
 \end{center}


\maketitle

%  {Rafael A. Rosales}\\
%  {\footnotesize\it Deparatmento de Computa\c{c}\~ao e Matem\'atica, 
%   Universidade de S\~ao Paulo}\\ 
%  {\footnotesize\it Av. Bandeirantes, 3900, Ribeir\~ao Preto, 
%  14049-901, SP, Brazil}\\{\footnotesize\verb+rrosales@usp.br+}\\[1em]
%%
%  {Rodrigo Drummond, Renan Valieris}\\
%  {\footnotesize\it Laboratory of Bioinformatics and Computational 
%  Biology, CIPE/A.C. Camargo Cancer Center\\ S\~ao Paulo 
%  01509-010,  Brazil}\\
%  {\footnotesize\verb+rddrummond@gmail.com+,    
%     \verb+rvalieris@gmail.com+}\\[1em] 
%%
%  {Israel T. da Silva}\\
%  {\footnotesize\it Laboratory of Bioinformatics and Computational 
%   Biology, CIPE/A.C. Camargo Cancer Center\\ S\~ao Paulo 
%   01509-010, Brazil}\\
%  {\footnotesize\it and}\\
%  {\footnotesize\it Laboratory of Molecular Immunology, 
%  The Rockefeller University}\\
%  {\footnotesize\it 1230 York Avenue, New York, NY 10065}\\
%  {\footnotesize\verb+itojal@gmail.com+}\\[1.5em]
%\end{center} 

\tableofcontents 

\section{Notation and preliminary Results}
Let $M$ be matrix of mutation counts of dimension $K\times G$ and let
$m$ denote a particular sample for $M$. For the factorization $M=PE$,
the observations $(M)_{ij}$ are independent and Poisson distributed
random variables with rates $(PE)_{ij}(W)_{ij}$, with $W$ as the
$K\times G$ opportunity matrix. The factors $P$, $E$, identified as
the model parameters are denoted as $\theta$. The likelihood for this
model, i.e. the function $\txcal L(\theta; m)$ defined by the map
$\theta \mapsto p(M=m|\theta)$, is given by
\begin{equation}
  \label{eqn:PoisLik}
   \txcal L(\theta; m) 
   =
    \prod_{i=1}^K \prod_{j=1}^G e^{-w_{ij}\sum_{n=1}^{N}p_{in}e_{nj}}
    \Big(w_{ij}\sum_{n=1}^{N}p_{in}e_{nj}\Big)^{m_{ij}}
    \frac{1}{m_{ij}!}.\tag{$s_1$}
\end{equation}

Posterior inferences about the factors $P$ and $E$ and the
factorization rank $N$ require the joint distribution for the
observations $M$ and the latent variables $Z$. Following the condition (1) in the main text and the independence between
the components of $Z$ this distribution equals
\begin{equation}
   \label{eqn:jointdata}
 \begin{aligned}
    p(Z = z, M =&\ m \mid P, E) \notag\\
  &= 
    \prod_{n=1}^N\prod_{i=1}^K\prod_{j=1}^G p\Big(Z_n = z_{inj}, M =
    m_{ij}, \mathbf{1}_{\big\{M_{ij} = \sum_{n=1}^N
      Z_{inj}\big\}} \Big| P, E\Big)  \notag\\ 
  &=
    \prod_{n=1}^N\prod_{i=1}^K\prod_{j=1}^G e^{-p_{in}e_{nj}w_{ij}}
    (p_{in}e_{nj}w_{ij})^{z_{inj}} \frac{1}{z_{inj}!}
    \mathbf{1}_{\big\{m_{ij} = \sum_{n=1}^N z_{inj}\big\}}.
 \end{aligned}
 \tag{$s_2$}
\end{equation}
The symbol $\mathbf{1}$ denotes the indicator function for the 
event $\big\{M_{ij} = \sum_n Z_{inj}\big\}$, with value 1 if $M_{ij} =  
\sum_n  Z_{inj}$ and 0 otherwise.


\section{Gibbs sampler}
\label{sec:Gibbs}
The construction of the Gibbs sampler relies on the determination of
the full conditional distributions for all unknowns in the
hierarchical model, that is, the distribution of each component of
$(Z, \theta, \psi)$ conditioned on all other components of this vector,
the data $M$ and the hyperprior parameters $\eta$. These distributions
are obtained by straightforward computations following the likelihood
in (\ref{eqn:PoisLik}) and the hierarchical model defined in the main
text.


The full conditional of $p_{in}$, for any $1 \leq i\leq K$ and $1\leq
n\leq  N$ is the following gamma density,
\begin{equation}
  \label{eqn:Full_for_P}
  p_{in} 
       \sim 
     \text{Gamma}\Big(p_{in}\,\Big|\, \alpha_{in}^p + 1 +
     \sum_{j=1}^G z_{inj}, \beta_{in}^p + \delta_p +
     \sum_{j=1}^G e_{nj}w_{ij}\Big).\tag{$s_2$}
\end{equation}
Similarly, the full conditional for $e_{nj}$, for any $1\leq n\leq 
N$, $1\leq j\leq G$, follows the Gamma density  
\begin{equation}
  \label{eqn:Full_for_E}
  e_{nj} 
     \sim 
   \text{Gamma}\Big(e_{nj}\,\Big|\, \alpha_{nj}^e + 1 +
   \sum_{i=1}^K z_{inj}, \beta_{nj}^e + \delta_e +
   \sum_{i=1}^K p_{in}w_{ij}\Big).\tag{$s_3$}
\end{equation}


As part of the  empirical Bayesian approach, we also consider the
following full conditional distributions for the hyperparameters
$\psi$. Up to proportionality, each entry of the $B_p$ matrix,
$\beta_{in}^p$,  has the full conditional density
\begin{equation}
 \label{eqn:Full_for_Bp}
 \beta_{in}^p
   \propto
      (\beta_{in}^p + \delta_p)^{\alpha_{in}^p + 1}(\beta_{in}^p)^{a_p
      - 1} \exp\Big(-(p_{in}+b_p)\beta_{in}^p\Big),\tag{$s_4$}
\end{equation}
for $\beta_{in}^p > 0$. When $\delta_p = 0$, this  corresponds  to a
Gamma density with shape $\alpha_{in}^p + 1 + a_p$ and rate $p_{in} +
b_p$.  Likewise, for the $B_e$ matrix, the full conditional has density
\begin{equation}
 \label{eqn:Full_for_Be}
 \beta_{nj}^e
   \propto
      (\beta_{nj}^e + \delta_e)^{\alpha_{nj}^e + 1}(\beta_{nj}^e)^{a_e
      - 1} \exp\Big(-(e_{nj}+b_e)\beta_{nj}^e\Big),\tag{$s_5$}
\end{equation}
for $\beta_{nj}^e > 0$. Also, if $\delta_e = 0$, this density is a
Gamma with shape $\alpha_{nj}^e + 1+ a_e$ and rate $e_{nj}+b_e$. It
should be observed that the densities for the cases $\delta_p > 0$ and
$\delta_e > 0$ are not from a standard family of known densities. In
this case, samples from the associated distributions may be generated
by using Metropolis-Hastings steps. Further details concerning their
implementation are described in Section~\ref{sec:MHsteps} here.



The exponential prior for the elements of $A_p$ leads, up to
proportionality, to the following the full conditional density
for\footnote{RD: I know we went already along this, but is it safe to
  take the constant $\lambda_p(\beta_{in}^p + \delta_p)$ away from the
density for $\alpha_{in}^p$ --because it cancels when one divides by
the integral that will define the normalising constant?}  
$\alpha_{ij}^p$,
\begin{equation}
 \label{eqn:Full_for_Ap}
  \alpha_{in}^p 
  \sim 
  \lambda_p\frac{(\beta_{in}^p + \delta_p)}{\Gamma(\alpha_{in}^p + 1)} 
   \Big[(\beta_{in}^p + \delta_p)p_{in}
   e^{-\lambda_p}\Big]^{\alpha_{in}^p}, \tag{$s_6$}
   \quad  \alpha_{in}^p > 0.
\end{equation}
Similarly, up to proportionality, for the elements of $A_e$ we
have\footnote{RD: same here, should we take away the constant
  $\lambda_e(\beta_{nj}^e + \delta_e)$ in the density for
  $\alpha_{nj}^e$?}
\begin{equation}
 \label{eqn:Full_for_Ae}
  \alpha_{nj}^e 
  \sim
  \lambda_e\frac{(\beta_{nj}^e + \delta_e)}{\Gamma(\alpha_{nj}^e + 1)} 
   \Big[(\beta_{nj}^e + \delta_e)e_{nj}
   e^{-\lambda_e}\Big]^{\alpha_{nj}^e}, 
   \quad  \alpha_{nj}^e > 0. \tag{$s_7$}
\end{equation}
The full conditional distributions for the hyperparameters $A_p$ and
$A_e$ are not from a standard family. Samples from these distributions
are obtained in this case also  by considering Metropolis-Hastings
steps. These are presented in Section~\ref{sec:MHsteps} here.


The full conditional distribution for the latent variables $Z$ also
has to be determined. In particular, for the model considered
throughout, the distribution of $Z$ is determined by a multinomial
distributions. This result is precisely stated by the following
Lemma.

\begin{lemma} Conditionally on  $M= m$, the full conditional
  distribution for the latent variables $Z$ is the following product
  of multinomial laws 
\[
   p(Z = z\,|\, M=m, \theta, \psi, \eta) 
   = 
   \prod_{i=1}^K\prod_{j=1}^G {m_{ij} \choose z_{i1j}, \ldots,
     z_{iNj}} 
   \prod_{n=1}^N \phi_{inj}^{z_{inj}},
\]
with $\phi_{inj} = p_{in} e_{nj}/\sum_{r=1}^N p_{ir}e_{rj}$.
\end{lemma}

\begin{proof} 
Conditionally on $M = m$, $P$ and $E$, the distribution of $Z$ is
independent of $\psi$ and $\eta$, $p(Z\,|\, M$, $P$, $E$, $\psi$, $\eta)
= p(Z\,|\, M$, $P$, $E)$, because by definition $Z$ is a set of
Poisson random variables with rates determined by $P$ and $E$. The
required full conditional is determined by the ratio $p(Z\,|\,M, P, E)
= p(Z,  M\,|\, P,E)/p(M\,|\,P, E)$, namely by considering the ratio 
of (\ref{eqn:jointdata}) by (\ref{eqn:PoisLik}). Taking logarithms
leads to
\begin{align*}
    \ln p(Z|M, P, E) 
  =&
    \ln p(Z, M|P, E) - \txcal L(\theta; m) \\
  =&
    \sum_{i=1}^K \sum_{j=1}^G \sum_{n=1}^N z_{inj}
     \ln(p_{in}e_{nj}w_{ij}) - 
     p_{in}e_{nj}w_{ij} - \ln(z_{inj}!) +\ln \mathbf{1}_{\big\{M_{ij} =
     \sum_{n=1}^N Z_{inj}\big\}} \\ 
    & -\sum_{i=1}^K \sum_{j=1}^G m_{ij}\ln \sum_{u=1}^N
      p_{iu}e_{uj}w_{ij} - 
      \sum_{u=1}^N p_{iu}e_{uj}w_{ij} + \ln m_{ij}!.
\end{align*}
Direct simplifications obtained by  setting $m_{ij}$ equal to
$\sum_{n=1}^N  z_{inj}$ lead to  
\begin{align*}
    \ln p(Z|M, P, E) 
  =&
    \sum_{i=1}^K \sum_{j=1}^G\Big\{
       \sum_{n=1}^N \Big(
           z_{inj} \ln\frac{p_{in}e_{nj}}{\sum_{u=1}^N
           p_{iu}e_{uj}} - \ln z_{inj}!
       \Big) + 
       \ln \mathbf{1}_{\big\{M_{ij} = \sum_{n=1}^N Z_{inj}
     \big\}} \\
     &+ \ln \big(\sum_{u=1}^N z_{iuj}\big)!
   \Big\}.
\end{align*}
Reverting to the original scale shows that the full conditional for
$Z$ is fact a product of multinomial distributions as required
\begin{align}
       p(Z = z\mid M = m, P, E) 
     &= 
       \prod_{i=1}^K \prod_{j=1}^G
       p(Z_{i1j} = z_{i1j}, \ldots, Z_{iNj} =
       z_{iNj}\mid m_{ij}, P, E) \notag\\ 
     &=
       \prod_{i=1}^K \prod_{j=1}^G
       \frac{m_{ij}!}{z_{i1j}!\cdots z_{iNj}!}
        \phi_{i1j}^{z_{i1j}} \cdots
       \phi_{iNj}^{z_{iNj}}. \notag\qedhere
\end{align}
\end{proof}


\section{MCMC implementation details}\label{sec:MHsteps}
The full conditional distributions for the entries in both $A_p$ and
$A_e$ do not have a standard form. These are explicitly shown in
(\ref{eqn:Full_for_Ap}) and (\ref{eqn:Full_for_Ae}). Draws from these
distributions, necessary to define the Gibbs sampler,  are obtained by
using Metropolis-Hastings steps.  Let $x > 0$ be the current value for
for any given entry of $A_e$ or $A_p$. A new candidate value $y$ for
this variable is generated from a gamma proposal density, $g(\cdot|
x)$ with shape $(x/\sigma)^2$ and rate $x/\sigma^2$. The mean of this
proposal is $x$ and its variance equals $\sigma^2$. Most results
described in the main text where obtained by setting $\sigma^2 =
*$\footnote{RD: please put the value here!}. Let $\alpha$ be
the ratio,
\[
  \alpha 
 =
  \frac{p(y)}{p(x)} \frac{g(x|y)}{g(y|x)}
\]
with $p(x)$ any one of the densities in (\ref{eqn:Full_for_Ap}) and
(\ref{eqn:Full_for_Ae}). The value $y$ is accepted as a sample
with probability $\rho = \min\{1, \alpha\}$, that is, if $x'$ denotes
the new sampled value, then
\[
   x'
    =
  \begin{cases}
    x, & \text{if } U > \rho,\\
    y, & \text{if } U \leq \rho.
  \end{cases}
\]

The samples for the entries in $B_p$ and $B_e$ are produced exactly in
the same way if $\delta_p > $ \textcolor{red}{Finish this!}.

\section{MCMC EM}
\subsection{Maximisation step}
This section describes the maximization step in the EM algorithm
necessary to derive the estimator $\hat\eta$, namely
\begin{equation}
   \label{eqn:etaMAX}
    \underset{\eta\,\in\,\Lambda}{\text{arg max}}\,
    \frac{1}{R}\sum_{r=1}^R \ln p\big(M, Z^{(r)}, \theta^{(r)},
    \psi^{(r)}\,|\, \eta\big). \tag{$s_*$}
\end{equation}
The solution to (\ref{eqn:etaMAX}) is used to define the sequence
$\eta^{(k)}$, $k \geq 1$. The components of the latter are denoted
throughout as $\lambda_a^{(k)}$, $a_e^{(k)}$ and so on.  Let
$\ell(\eta) = \ln p(M, Z^{(r)}, \theta^{(r)}, \psi^{(r)}\,|\,
\eta)$. The maximization of $\frac{1}{R}\sum_{r}\ell(\eta)$ with
respect to $\eta$ is made by solving
$\nabla\frac{1}{R}\sum_{r}\ell(\eta) = 0$ for $\eta$.  To start,
observe that $\ell(\eta)$ admits the form
\begin{equation}
 \label{eqn:ell}
 \begin{aligned}
  \ell(\eta)
  =&
  \ln p(M, Z^{(r)} | \theta^{(r)})  + \ln p(P^{(r)}\,|\, A_p^{(r)},
  B_p^{(r)}) + \ln p(E^{(r)}\,|\, A_e^{(r)},  B_e^{(r)})  \\
  &+
  \ln p(A_p^{(r)}\,|\, \eta) + \ln p(B_p^{(r)}\,|\, \eta) +
  \ln p(A_e^{(r)}\,|\, \eta) + \ln p(B_e^{(r)}\,|\, \eta).
 \end{aligned}
 \tag{$s_{**}$}
\end{equation}
The first three terms of $\ell(\eta)$ are independent of
$\eta$. Further, each of the remaining terms can be optimized
separately because each one depends on a different component of
$\eta$. Following the exponential hyperprior for $A_p^{(r)}$ we have
\[
   \frac{1}{R}\sum_{r=1}^R \ln  p(A_p^{(r)}\,|\ \lambda_p)
  %=
  % \frac{1}{R}\sum_{r=1}^R \sum_{i=1}^K\sum_{n=1}^N
  % \big(\ln(\lambda_p) - \lambda_p (A_p^{(r)})_{in}\big)
  = 
  \frac{1}{R}KN\ln(\lambda_p) - \frac{1}{R}\lambda_p\sum_{r=1}^R
  \sum_{i=1}^K\sum_{n=1}^N (A_p^{(r)})_{in}.
\]
Hence, solving for $\lambda_p$ in $\partial (\frac{1}{R}\sum_r \ln
p(A_p^{(r)}\,|\ \lambda_p))/\partial\lambda_p = 0$ gives
\[
  \lambda_p^{(k)} = \frac{RKN}{\sum_{r=1}^R \sum_{i=1}^K 
    \sum_{n=1}^N (A_p^{(r)})_{in}}.
\]
Likewise, by considering the third of the remaining terms in
(\ref{eqn:ell}), namely $\frac{1}{R} \sum_r 
p(A_e\,|\, \lambda_e)$, gives
\[
  \lambda_e^{(k)} = \frac{RNG}{\sum_{r=1}^R \sum_{n=1}^N
    \sum_{j=1}^G (A_e^{(r)})_{nj}}.
\]
The maximization of the second and the fourth terms in (\ref{eqn:ell})
must be handled numerically. This situation is analogous to the
maximum likelihood estimation of the scale and the rate parameters of
a Gamma density, which has no closed form solution, see \cite{CW}. For
instance, because of a Gamma hyperprior for $B_p$ with parameters
$a_p$ and $b_p$, for the second term we have that
\[
   \frac{1}{R}\sum_{r=1}^R \ln p(B_p^{(r)}\,|\, \eta)
  =
   \frac{1}{R}\sum_{r=1}^R \sum_{i}\sum_{n} a_p\ln b_p  
    - \ln\Gamma(a_p) + 
  (a_p-1)\ln\big[(B_p^{(r)})_{in} - b_p(B_p^{(r)})_{in}\big].
\]
Solving for $b_p$ in $\partial(\frac{1}{R}\sum_r \ln p(B_p^{(r)}\,|\,
\eta))/\partial b_p = 0$  yields\footnote{RD, RR: have to check this
  last equation!}
\[
   b_p =\frac{1}{R} \frac{\sum_r\sum_i\sum_n (B_p^{(r)})_{in}}{a_p}.
\] 
\textcolor{red}{Rodrigo: still to write} the numerical implementation 
  of how $b_p$ (and hence also $b_e$) are obtained. I know you use
  \cite{J}, anyway  please explain.


\subsection{Convergence}
The results in this section are adapted from those in \cite{C01}.

\begin{lemma}\label{lem:technical} For the hierarchical model in (*), the 
  following holds,
\begin{itemize}
 \item[(i)] $\hat\eta^{(k)} \to \eta$ as $k \to \infty$ almost
   surely with respect to $m$; 
 \item[(ii)] $h_\eta(\eta')$ is continuous in both $\eta$ and $\eta'$;
   were, 
\[
  h_{\eta}(\eta) 
   = 
 \int p(\theta|M, Z, \psi, \eta')p(\psi|M, \eta)\ d\psi
\]
\item[(iii)] $\hat h_\eta(\eta')$, defined as
\[
  \hat h_\eta(\eta') 
  = 
 \frac{1}{R}\sum_{r=1}^R \pi(\theta|M, Z^{(r)}, \psi^{(r)}, \eta')
    \quad\text{with}\quad 
   \psi^{(r)} \sim \pi(\psi|M, Z^{(r)}, \theta^{(r)}, \eta)
\]
 is continuous in $\eta'$ and stochastically equicontinuous in $\eta$,
that is, for any given $\epsilon > 0$ there is $\delta>0$ such that
$|\eta_1 - \eta_2| < \delta$ implies $|\hat h_{\eta_1}(\eta') - \hat
h_{\eta_2}(\eta')|$ for all $\eta_1, \eta_2$ except for a set of 0
$\pi$-measure.
 \item[(iv)] the Gibbs sampler produces an ergodic Markov chain.
\end{itemize}
\end{lemma}
\begin{proof}  The first assertion follow by the argument exposed in
  the main text, namely ... The second assertion follows by the
  continuity of the full conditional densities for $\theta$ and
  $\psi|M, \eta$ with respect to $\eta'$ and $\eta$
  respectively. Likewise, (iii) follows by ... The ergodicity of our
  hybrid Gibbs sampler is ensured by ...
\end{proof}

\begin{theorem} Under the conditions in Lemma~\ref{lem:technical}, for
  each measurable set $T\subseteq\Theta$, as $R$ and $k \to \infty$,
\[
  \int_T \bigg|
       \frac{1}{R}\sum_{r=1}^R \pi\big(\theta|M, Z^{(r)}, \psi^{(r)},
       \hat\eta^{(k)}\big) - \pi(\theta|M, \eta)
    \bigg|\ d\theta \to 0
\]
almost surely with respect to the measures with densities $g$  and
$m$.
\end{theorem}
\begin{proof}
This Theorem is a immediate consequence of Lemma~\ref{lem:technical}
and Scheff\'e's Lemma, see \cite{C01} for further details.
\end{proof}


\section{Installing and running \texttt{signeR}}
\subsection{Installing \texttt{signeR}}
\texttt{signeR} is available as an R package and it can be installed
from R's prompt by typing
\begin{lstlisting}[]
  install.packages('signeR', dependencies=TRUE)
\end{lstlisting}
This should install also the following dependencies: \textcolor{red}{....  Now comes
the difficult bit: are we distributing pre-compiled binaries at CRAN,
or just source? Note that the first option needs someone to mantain
this over time. In the second case, the installation relies upon the
compilation of C++ code, and hence depends on the existence of a
properly working C compiler. Have to explain on how to do this in a
Mac OS X and Windows.} 


\subsection{Running \texttt{signeR}}
Write: A step-by-step example about how \texttt{signeR} would be run
on a real data example, probably on the 21 breast cancer data.
Loading the package \verb+library(signeR)+ and then typing
\verb+eBayesNMF()+ with the following arguments. Explain the data
structure for the input matrices $M$, $W$.


Mention the actual dependencies \cite{Boo}, etc, and the also
R's NMF package.

\begin{lstlisting}[]
  # this is a comment
  Mut<-read.table('21_breast_cancers.mutations.txt',header=FALSE)
  Opp<-read.table('21_breast_cancers.opportunity.txt',header=FALSE)
  Outsig<-signeR(M=Mut, Opport=Opp)  
  Paths(Outsig$SignExp)
  SignPlot(Outsig$SignExp)
  SignBoxPlot(Outsig$SignExp)
  Classify(Outsig$SignExp, labels=c(rep(`GradeII',10),
     rep(`GradeIII',10), NA))
  DiffExp(Outsig$SignExp, labels=c(rep(`GradeII',10),
     rep(`GradeIII',11)))
\end{lstlisting}

%\bibliographystyle: natbib, achemnat, plainnat, abbrv, plain 
\bibliographystyle{alpha}
\bibliography{suppl}


\vspace{2.5cm}

{\footnotesize
\begin{tabular}{ll}
 \centering
  \parbox[c][4cm][t]{8cm}{
   {\sc Rafael A. Rosales}\\
   {\it 
   Departamento de Computa\c{c}\~ao e Matem\'atica\\
   Universidade de S\~ao Paulo\\
   Av. Bandeirantes, 3900, Ribeir\~ao Preto\\
   CEP 14049-901,SP Brazil\\}
   \verb~rrosales@usp.br~
  }
&
  \parbox[c][4cm][t]{6.6cm}{
  {\sc Rodrigo D. Drummond\\
       Renan Valieris}\\
    {\it 
    Laboratory of Bioinformatics and Computational\\
    Biology, CIPE/A.C. Camargo Cancer Center\\ 
    S\~ao Paulo 01509-010, Brazil\\}
 \verb~rddrummond@gmail.com~\\ \verb~rvalieris@gmail.com~
 }
\\
 \parbox[c][4cm][t]{6.6cm}{
   {\sc Israel T. da Silva}\\
   {\it 
    Laboratory of Bioinformatics and Computational\\
    Biology, CIPE/A.C. Camargo Cancer Center\\ 
    S\~ao Paulo 01509-010, Brazil\\
   and\\
   Laboratory of Molecular Immunology\\
   The Rockefeller University\\
   1230 York Avenue, New York, NY 10065\\}
  \verb~itojal@gmail.com~
 }
\end{tabular}
}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

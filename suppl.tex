\documentclass[11pt]{amsart}  
\usepackage{amsmath, amssymb, amsthm}
\usepackage[marginratio = 1:1,
     height = 601pt, 
     width = 420pt,
     tmargin = 95pt]{geometry}
\usepackage{listings}
 \lstset{language=R, 
     showspaces=false,
     showtabs=false,
     showstringspaces=false,
     basicstyle=\ttfamily
     \small,
     backgroundcolor=\color{mygray},
     frame=single,
     framerule=0.0pt}
\usepackage{graphicx}
\usepackage[nomarkers,figuresonly]{endfloat} 
 \renewcommand\thefigure{S.\arabic{figure}}
\usepackage{xcolor}
\newcommand{\collineB}{0,0.3,0.8} 
\definecolor{mycolB}{rgb}{\collineB}
\definecolor{mygray}{rgb}{1,1,1}
\usepackage[colorlinks, 
     linkcolor=mycolB, 
     citecolor=blue,
     urlcolor=blue]{hyperref}
\usepackage{verbatim}
% $$$:
\renewcommand{\rmdefault}{ptm} % times
\renewcommand*\ttdefault{txtt} % typewriter is txtt
\renewcommand*\sfdefault{phv}  % helvetica
\usepackage[subscriptcorrection]{mtpro}
\usepackage[mtphrb]{mtpams}
\usepackage[mtpcal, mtpfrak]{mtpb}
%\usepackage[scaled=1.1]{rsfso}
%
%\DeclareMathAlphabet{\txcal}{U}{tx-cal}{m}{n}
\DeclareMathAlphabet{\rsocl}{U}{rsfso}{m}{n}
%\DeclareFontEncoding{FMS}{}{}
%  \DeclareFontSubstitution{FMS}{futm}{m}{n}
%\DeclareMathAlphabet{\foucal}{FMS}{futm}{m}{n}
%
\newcommand{\wP}{P^\ast}
\newcommand{\wE}{E^\ast}
\newcommand{\wZ}{Z^\ast}
\newcommand{\wt}{\theta^\ast}
\newcommand{\wk}{\psi^\ast}
\newcommand{\whp}{\widehat \pi}
\newcommand{\wAe}{A_e^\ast}
\newcommand{\wAp}{A_p^\ast}
\newcommand{\wBe}{B_e^\ast}
\newcommand{\wBp}{B_p^\ast}
%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
%\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\setcounter{tocdepth}{3} % to get subsubsections in toc
\let\oldtocsection=\tocsection
\let\oldtocsubsection=\tocsubsection
\let\oldtocsubsubsection=\tocsubsubsection
\renewcommand{\tocsection}[2]{\hspace{0em}\oldtocsection{#1}{#2}}
\renewcommand{\tocsubsection}[2]{\hspace{.9em}\oldtocsubsection{#1}{#2}}
\renewcommand{\tocsubsubsection}[2]{\hspace{.5em}\oldtocsubsubsection{#1}{#2}}

%\makeatletter
%\def\@tocline#1#2#3#4#5#6#7{\relax
%  \ifnum #1>\c@tocdepth % then omit
%  \else
%    \par \addpenalty\@secpenalty\addvspace{#2}%
%    \begingroup \hyphenpenalty\@M
%    \@ifempty{#4}{%
%      \@tempdima\csname r@tocindent\psimber#1\endcsname\relax
%    }{%
%      \@tempdima#4\relax
%    }%
%    \parindent\z@ \leftskip#3\relax \advance\leftskip\@tempdima\relax
%    \rightskip\@pnumwidth plus4em \parfillskip-\@pnumwidth
%    #5\leavevmode\hskip-\@tempdima
%      \ifcase #1
%       \or\or \hskip .5em \or \hskip 2em \else \hskip 3em \fi%
%      #6\nobreak\relax
%    \dotfill\hbox to\@pnumwidth{\@tocpagenum{#7}}\par
%    \nobreak
%    \endgroup
%  \fi}
%\makeatother

\makeatletter
\def\@fnsymbol#1{%
  \ensuremath{%
    \ifcase#1% 0
    \or % 1
      \dagger%   
    \or % 2
      *
    \or % 3  
      \ddagger
    \or % 4   
      \mathsection
    \or % 5
      \mathparagraph
    \else % >= 6
      \@ctrerr  
    \fi
  }%   
}   
\makeatother

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}
\title[\textup{R. A. Rosales, R. D. Drummond, 
       R. Valieris, E. Dias-Neto, I. T. da Silva}]{} 
\noindent\parbox{\linewidth}{\footnotesize %
    {\footnotesize\textsl{Submitted to Bioinformatics }}
    {\footnotesize\textrm{\hfill\today}}
  }\\[1em]
 \begin{center}
   {\large Supplementary material to:}\\[1em]
   {\Large\sc An empirical Bayesian approach}\\[0.3em] 
   {\Large\sc to Mutational Signature Discovery}\\[2em]
   {\large 
     Rafael A. Rosales\footnote{R. A. Rosales and R. D. Drummond are
       both to be considered as First Author}, 
     Rodrigo D. Drummond$^\dagger$,
     Renan Valieris, Emmanuel Dias-Neto  and
     Israel T. da Silva\footnote{Corresponding
       author}$^,$\footnote{Partially supported by FAPESP grant */*}}
 \end{center}

\maketitle

\tableofcontents 

\section{Notation and preliminary Results}
Let $M$ be matrix of mutation counts of dimension $K\times G$ and let
$m$ denote a particular sample for $M$. For the factorization $M=PE$,
the observations $(M)_{ij}$ are independent and Poisson distributed
random variables with rates $(PE)_{ij}(W)_{ij}$, with $W$ as the
$K\times G$ opportunity matrix. The factors $P$, $E$, identified as
the model parameters are denoted as $\theta$. The same as 
\cite{FICMV}, we regard the opportunity matrix $W$ as fixed and known
and set $w_{ij} = 1$ for all $i$ and $j$ if $W$ is not determined out
from the data. The likelihood for this model, i.e. the function
$\rsocl L: \Theta \to \mathbb R$ defined by $\theta \mapsto p(M=m|\theta)$, is given by 
\begin{equation}
  \label{eqn:PoisLik}
   \rsocl L(\theta; m) 
   =
    \prod_{i=1}^K \prod_{j=1}^G e^{-w_{ij}\sum_{n=1}^{N}p_{in}e_{nj}}
    \Big(w_{ij}\sum_{n=1}^{N}p_{in}e_{nj}\Big)^{m_{ij}}
    \frac{1}{m_{ij}!}.\tag{$s_1$}
\end{equation}

Posterior inferences about the factors $P$ and $E$ and the
factorization rank $N$ require the joint distribution for the
observations $M$ and the latent variables $Z$. This is also known as
the complete data likelihood function when interpreted as a function
of  $\theta$ for given $Z=z$ and $M=m$. Following the condition 
(1) in the main text and the independence between
the components of $Z$, this distribution equals
\begin{equation}
   \label{eqn:jointdata}
 \begin{aligned}
    p(Z = z, M =&\ m \mid P, E) \notag\\
  &= 
    \prod_{n=1}^N\prod_{i=1}^K\prod_{j=1}^G p\Big(Z_{inj} = z_{inj},
    M_{ij} = m_{ij}, \mathbf{1}_{\big\{M_{ij} = \sum_{n=1}^N
      Z_{inj}\big\}} \Big| P, E\Big)  \notag\\ 
  &=
    \prod_{n=1}^N\prod_{i=1}^K\prod_{j=1}^G e^{-p_{in}e_{nj}w_{ij}}
    (p_{in}e_{nj}w_{ij})^{z_{inj}} \frac{1}{z_{inj}!}
    \mathbf{1}_{\big\{m_{ij} = \sum_{n=1}^N z_{inj}\big\}}.
 \end{aligned}
 \tag{$s_2$}
\end{equation}
The symbol $\mathbf{1}$ denotes the indicator function for the  
event $\big\{M_{ij} = \sum_n Z_{inj}\big\}$, with value 1 if $M_{ij} =  
\sum_n  Z_{inj}$ and 0 otherwise. Marginalization of
(\ref{eqn:jointdata}) with respect to $Z$ gives  (\ref{eqn:PoisLik}).


\section{Gibbs sampler}
\label{sec:Gibbs}
The construction of the Gibbs sampler relies on the determination of
the full conditional distributions for all unknowns in the
hierarchical model, that is, the distribution of each component of
$(Z, \theta, \psi)$ conditioned on all other components of this vector,
the data $M$ and the hyperprior parameters $\eta$,
\[
   Z \sim \pi(Z|\theta, \psi, M, \eta), \quad
   \theta \sim \pi(\theta|Z, \psi, M, \eta) \quad
  \text{and}\quad
  \psi \sim \pi(\psi|Z, \theta, M, \eta).
\]
The conditional distributions for $\theta$ and $\psi$ are obtained by
straightforward computations following the likelihood in
(\ref{eqn:PoisLik}) and the hierarchical model defined througout the
main text. Only the conditional for $Z$, which also depends on the
complete data likelihood in (\ref{eqn:jointdata}), deserves special
attention. The conditional distribution for $Z$ is derived in
Lemma~\ref{lem:Full_for_Z}.


To begin with, by \emph{a priori} indepence between $P$ and $E$, for
the conditional for $\theta$ one has that 
\[
   \pi(\theta|Z, \psi, M, \eta) 
   = 
   \prod_{i=1}^K\prod_{n=1}^N\prod_{j=1}^G
    \pi(p_{in}|Z, \psi, M, \eta)
    \pi(e_{nj}|Z, \psi, M, \eta).
\]
Direct computations show that, for any $1 \leq i\leq K$ and $1\leq
n\leq  N$, $p_{in}$ has the following Gamma density
\begin{equation}
  \label{eqn:Full_for_P}
  p_{in} 
       \sim 
     \text{Gamma}\Big(p_{in}\,\Big|\, \alpha_{in}^p + 1 +
     \sum_{j=1}^G z_{inj}, \beta_{in}^p +
     \sum_{j=1}^G e_{nj}w_{ij}\Big).\tag{$s_2$}
\end{equation}
Similarly, the full conditional for $e_{nj}$, for any $1\leq n\leq 
N$, $1\leq j\leq G$, follows the Gamma density  
\begin{equation}
  \label{eqn:Full_for_E}
  e_{nj} 
     \sim 
   \text{Gamma}\Big(e_{nj}\,\Big|\, \alpha_{nj}^e + 1 +
   \sum_{i=1}^K z_{inj}, \beta_{nj}^e +
   \sum_{i=1}^K p_{in}w_{ij}\Big).\tag{$s_3$}
\end{equation}


Due to \emph{a priori} independence between the
components of the matrices $A_e$, $B_e$, $A_p$ and $B_p$, the full
conditional distribution for the hyperparameters $\psi$ equals the
product
\[
   \prod_{i=1}^K\prod_{n=1}^N
     p(\alpha_{in}^p| \theta, Z, M, \eta)
     p(\beta_{in}^p|\theta, Z, M, \eta)
   \prod_{n=1}^N\prod_{j=1}^G
     p(\alpha_{nj}^e|\theta, Z, M, \eta)
     p(\beta_{nj}^e|\theta, Z, M, \eta).
\]
The update for $\psi$ runs therefore through the update of each
element of these matrices. Up to proportionality, each entry of the
$B_p$ matrix, $\beta_{in}^p$,  has the full conditional density
\begin{equation}
 \label{eqn:Full_for_Bp}
 \beta_{in}^p
   \propto
   (\beta_{in}^p)^{a_p + \alpha_{in}^p} 
   \exp\Big(-(p_{in}+b_p)\beta_{in}^p\Big),\tag{$s_4$}
\end{equation}
for $\beta_{in}^p > 0$. This corresponds to a Gamma density with shape
$\alpha_{in}^p + 1 + a_p$ and rate $p_{in} + b_p$.  Likewise, for each
entry of the $B_e$ matrix, the full conditional has density 
\begin{equation}
 \label{eqn:Full_for_Be}
 \beta_{nj}^e
   \propto
  (\beta_{nj}^e)^{a_e + \alpha_{nj}^e} 
  \exp\Big(-(e_{nj}+b_e)\beta_{nj}^e\Big),\tag{$s_5$}
\end{equation}
which is a  Gamma density for $\beta_{nj}^e > 0$, with shape
$\alpha_{nj}^e + 1+ a_e$ and rate $e_{nj}+b_e$. 


The exponential prior for the elements of $A_p$ leads, up to
proportionality, to the following the full conditional density
for $\alpha_{ij}^p$,
\begin{equation}
 \label{eqn:Full_for_Ap}
  \alpha_{in}^p 
  \sim 
  \lambda_p\frac{\beta_{in}^p}{\Gamma(\alpha_{in}^p + 1)} 
   \Big[\beta_{in}^pp_{in}
   e^{-\lambda_p}\Big]^{\alpha_{in}^p}, \tag{$s_6$}
   \quad  \alpha_{in}^p > 0.
\end{equation}
Similarly, up to proportionality, for the elements of $A_e$ we
have
\begin{equation}
 \label{eqn:Full_for_Ae}
  \alpha_{nj}^e 
  \sim
  \lambda_e\frac{\beta_{nj}^e}{\Gamma(\alpha_{nj}^e + 1)} 
   \Big[\beta_{nj}^e e_{nj}
   e^{-\lambda_e}\Big]^{\alpha_{nj}^e}, 
   \quad  \alpha_{nj}^e > 0. \tag{$s_7$}
\end{equation}
The full conditional distributions for the hyperparameters $A_p$ and
$A_e$ are not from a standard family. Samples from these distributions
are obtained by considering Metropolis-Hastings steps. The
implementation of these is described in Section~\ref{sec:MHsteps}.

The full conditional distribution for the latent variables, $Z$, is a
product of multinomial distributions. This result is precisely stated
by the following Lemma. 

\begin{lemma}\label{lem:Full_for_Z} Conditionally on  $M= m$, the full
  conditional distribution for the latent variables $Z$ is a  product
  of multinomial laws 
\[
   p(Z = z\,|\, \theta, \psi, M=m, \eta)
   = 
   \prod_{i=1}^K\prod_{j=1}^G {m_{ij} \choose z_{i1j}, \ldots,
     z_{iNj}} 
   \prod_{n=1}^N \phi_{inj}^{z_{inj}},
\]
with $\phi_{inj} = p_{in} e_{nj}/\sum_{r=1}^N p_{ir}e_{rj}$.
\end{lemma}

\begin{proof} 
Conditionally on $M = m$, $P$ and $E$, the distribution of $Z$ is
independent of $\psi$ and $\eta$ because by definition of our model
$Z$ is a set of Poisson random variables with rates determined by $P$
and $E$. The required full conditional is therefore determined by the
ratio $p(Z\,|\,M, P, E) = p(Z, M\,|\, P,E)/p(M\,|\,P, E)$, that is by
considering the ratio of (\ref{eqn:jointdata}) by
(\ref{eqn:PoisLik}). Taking logarithms leads to
\begin{align*}
    \ln p(Z|M, P, E) 
  =&
    \ln p(Z, M|P, E) - \rsocl L(\theta; m) \\
  =&
    \sum_{i=1}^K \sum_{j=1}^G \sum_{n=1}^N z_{inj}
     \ln(p_{in}e_{nj}w_{ij}) - 
     p_{in}e_{nj}w_{ij} - \ln(z_{inj}!) \\
    & + \ln \mathbf{1}_{\big\{M_{ij} =
     \sum_{n=1}^N Z_{inj}\big\}}
     -\sum_{i=1}^K \sum_{j=1}^G m_{ij}\ln \sum_{u=1}^N
      p_{iu}e_{uj}w_{ij} \\
    & - \sum_{u=1}^N p_{iu}e_{uj}w_{ij} + \ln m_{ij}!.
\end{align*}
Direct simplifications obtained by  setting $m_{ij}$ equal to
$\sum_{n=1}^N  z_{inj}$ give
\begin{align*}
    \ln p(Z|M, P, E) 
  =&
    \sum_{i=1}^K \sum_{j=1}^G\Big\{
       \sum_{n=1}^N \Big(
           z_{inj} \ln\frac{p_{in}e_{nj}}{\sum_{u=1}^N
           p_{iu}e_{uj}} - \ln z_{inj}!
       \Big) + 
       \ln \mathbf{1}_{\big\{M_{ij} = \sum_{n=1}^N Z_{inj}
     \big\}} \\
     &+ \ln \big(\sum_{u=1}^N z_{iuj}\big)!
   \Big\}.
\end{align*}
Taking exponential to revert to the original scale shows that
the full conditional for $Z$ is a product of multinomial
distributions as required,
\begin{align}
       p(Z = z\mid M = m, P, E) 
     &= 
       \prod_{i=1}^K \prod_{j=1}^G
       p(Z_{i1j} = z_{i1j}, \ldots, Z_{iNj} =
       z_{iNj}\mid m_{ij}, P, E) \notag\\ 
     &=
       \prod_{i=1}^K \prod_{j=1}^G
       \frac{m_{ij}!}{z_{i1j}!\cdots z_{iNj}!}
        \phi_{i1j}^{z_{i1j}} \cdots
       \phi_{iNj}^{z_{iNj}}. \notag\qedhere
\end{align}
\end{proof}

\begin{remark} The result of Lemma~\ref{lem:Full_for_Z} holds if
$W=\mathbf 1$ or $W \in \mathbb R^{K\times G}_+$ and includes
therefore both modelling approaches without and with opportunities.
\end{remark}

\subsection{Metropolis-within-Gibbs steps}\label{sec:MHsteps}
The full conditional distributions for the entries in both $A_p$ and
$A_e$ do not have a standard form. These are explicitly shown in
(\ref{eqn:Full_for_Ap}) and (\ref{eqn:Full_for_Ae}). Draws from these
distributions, necessary to define the Gibbs sampler,  are obtained by
using Metropolis-Hastings steps.  Let $x > 0$ be the current value
for any given entry of $A_e$ (or $A_p$). A new candidate value $y$ for
this variable is generated from a Gamma proposal density, $g(\cdot|
x)$ with shape $(x/\sigma)^2$ and rate $x/\sigma^2$. The mean of this
proposal is thus set to equal $x$ and its variance to $\sigma^2$. The
results described in the main text were obtained by choosing
$\sigma^2 = 10$. Let $\ln \alpha$ be defined as
\[
  \ln \alpha 
 =
  \ln p(y) - \ln p(x) + \ln g(x|y) - \ln g(y|x)
\]
with $p(x)$ as the density in (\ref{eqn:Full_for_Ap}) (or in  
(\ref{eqn:Full_for_Ae})). The value $y$ is accepted as a sample
with probability $\rho = \min\{1, e^{\alpha}\}$, that is, if $x'$ denotes
the new sampled value, then
\[
   x'
    =
  \begin{cases}
    x, & \text{if } U > \rho,\\
    y, & \text{if } U \leq \rho.
  \end{cases}
\]
for $U$ an uniformly distributed random variable on $[0, 1]$.

\section{MCMC EM}
\subsection{Maximisation step}
This section describes the maximisation step in the EM algorithm
necessary to derive the estimator $\hat\eta$, namely
\begin{equation}
   \label{eqn:etaMAX}
    \underset{\eta\,\in\,\Lambda}{\text{arg max}}\,
    \frac{1}{R}\sum_{r=1}^R \ln \rsocl L\big(\eta; m, Z^{(r)},
    \theta^{(r)}, \psi^{(r)}\big). \tag{$s_9$} 
\end{equation}
As described in the main text, the solution to (\ref{eqn:etaMAX}) is
used to define the sequence $\hat\eta^{(u)}$, $u \geq 1$. The
components of the latter are denoted throughout as $\hat b_e^{(u)}$,
$\hat a_e^{(u)}$ and so on.  To simplify notation, let $\rsocl L(\eta;
m, Z^{(r)}, \theta^{(r)}, \psi^{(r)}) = \rsocl L(\eta)$. The
maximisation of $\frac{1}{R}\sum_{r} \ln \rsocl L(\eta)$ with respect
to $\eta$ is made by solving $\nabla\frac{1}{R}\sum_{r}\ln \rsocl
L(\eta) = 0$ for $\eta$.  To start, observe that $\ln \rsocl L(\eta)$
admits the form
\begin{equation}
 \label{eqn:ell}
 \begin{aligned}
  \ln \rsocl L(\eta)
  =&
  \ln p(M, Z^{(r)} | \theta^{(r)})  + \ln p(P^{(r)}\,|\, A_p^{(r)},
  B_p^{(r)}) + \ln p(E^{(r)}\,|\, A_e^{(r)},  B_e^{(r)})  \\
  &+
  \ln p(A_p^{(r)}\,|\, \eta) + \ln p(B_p^{(r)}\,|\, \eta) +
  \ln p(A_e^{(r)}\,|\, \eta) + \ln p(B_e^{(r)}\,|\, \eta).
 \end{aligned}
 \tag{$s_{10}$}
\end{equation}
The first three terms of are independent of
$\eta$. Further, each of the remaining terms can be optimised
separately because each one depends on a different component of
$\eta$. Following the exponential hyperprior for $A_p$ we have
\[
   \frac{1}{R}\sum_{r=1}^R \ln  p(A_p^{(r)}\,|\ \lambda_p)
  %=
  % \frac{1}{R}\sum_{r=1}^R \sum_{i=1}^K\sum_{n=1}^N
  % \big(\ln(\lambda_p) - \lambda_p (A_p^{(r)})_{in}\big)
  = 
  KN\ln(\lambda_p) - \frac{1}{R}\lambda_p
  \sum_{r=1}^R\sum_{i=1}^K\sum_{n=1}^N \big(A_p^{(r)}\big)_{in}.
\]
Hence, solving for $\lambda_p$ in 
\[
  \frac{\partial}{\partial\lambda_p} \frac{1}{R}\sum_r \ln
  p(A_p^{(r)}\,|\ \lambda_p)) = 0
\] 
gives
\[
  \widehat\lambda_p = \frac{RKN}{\sum_{r=1}^R \sum_{i=1}^K 
    \sum_{n=1}^N \big(A_p^{(r)}\big)_{in}}.
\]
Likewise, by considering the third of the remaining terms in
(\ref{eqn:ell}), that is, $\frac{1}{R} \sum_r 
p(A_e\,|\, \lambda_e)$, gives
\[
  \widehat\lambda_e = \frac{RNG}{\sum_{r=1}^R \sum_{n=1}^N
    \sum_{j=1}^G \big(A_e^{(r)}\big)_{nj}}.
\]
The maximization of the second and the fourth terms in (\ref{eqn:ell})
must be handled numerically. This situation is analogous to the
maximum likelihood estimation of the scale and the rate parameters of
a Gamma density, which has no closed form solution, see \cite{CW}. To
circumvent this difficulty *****


For
instance, because of a Gamma hyperprior for $B_p$ with parameters
$a_p$ and $b_p$, for the second term we have that
\[
   \frac{1}{R}\sum_{r=1}^R \ln p(B_p^{(r)}\,|\, \eta)
  =
   \frac{1}{R}\sum_{r=1}^R \bigg(\sum_{i}\sum_{n} a_p\ln b_p  
    - \ln\Gamma(a_p) + 
   (a_p-1)\ln\big[\big(B_p^{(r)}\big)_{in}\big] -  
    b_p\big(B_p^{(r)}\big)_{in}\bigg).
\]
Solving for $b_p$ in 
\[
  \frac{\partial}{\partial b_p}
   \frac{1}{R}\sum_r \ln p(B_p^{(r)}\,|\, \eta) = 0
\]
yields
\[
   \hat b_p = \frac{RKN}{\sum_{r=1}^R \sum_{i=1}^K \sum_{n=1}^N 
     \big(B_p^{(r)}\big)}_{in}  a_p.
\]
Similarly, for $b_e$ we have
\[
   \hat b_e = \frac{RNG}{\sum_{r=1}^R \sum_{n=1}^N  \sum_{j=1}^G 
     \big(B_e^{(r)}\big)}_{nj}  a_e.
\]
\textcolor{red}{Rodrigo: still to write the numerical implementation
of how $a_p$ (and hence also $a_e$) are obtained. Maybe you are using
\cite{J}, in any case please explain.}


\subsection{Convergence}
This section justifies the convergence of $\widehat\pi(\theta|M,
\hat\eta)$ as defined by (3) towards $\pi(\theta|M, \eta)$. The
arguments follow closely those in \cite{C01} but they are
adapted to the hierarchical NMF model considered throughout. 
\begin{lemma}\label{lem:technical} Suppose that the following
  conditions hold:
\begin{itemize}
 \item[(i)] $\hat\eta^{(u)} \to \eta$ as $u \to \infty$ almost
   surely with respect to $m$; 
 \item[(ii)] $h_\eta(\eta')$ defined as
\[
  h_{\eta}(\eta') 
   = 
 \int p(\theta|Z, \psi, M, \eta')p(Z, \psi|M, \eta)\
 \textup{d} Z \textup{d} \psi
\]
is continuous in both $\eta$ and $\eta'$;
\item[(iii)] $\widehat h_\eta(\eta')$, defined as
\begin{align*}
  \widehat h_\eta(\eta') 
  = 
 \frac{1}{R}\sum_{r=1}^R \pi(\theta|Z^{(r)}, \psi^{(r)}, M, \eta')
    \quad \text{with}\quad 
   \begin{aligned}
      &\psi^{(r)} \sim \pi(\psi|Z^{(r-1)}, \theta^{(r)}, M, \eta)\\
      &Z^{(r)} \sim \pi(Z| \psi^{(r)}, \theta^{(r)}, M, \eta) 
   \end{aligned}
\end{align*}
 is continuous in $\eta'$ and stochastically equicontinuous in $\eta$,
that is, for any given $\epsilon > 0$ there is $\delta>0$ such that
$|\eta_1 - \eta_2| < \delta$ implies $|\widehat h_{\eta_1}(\eta') -
\widehat h_{\eta_2}(\eta')|$\ \ for all $\eta_1, \eta_2$ except for a
$\pi$-measurable null set.
 \item[(iv)] the Metropolis-within-Gibbs sampler produces an ergodic
   Markov chain. 
\end{itemize}
Then there exists a subsequence $(r_u)$ such that $\lim_{u\to\infty}
r_u  = \infty$, for which
\[
   \big|\widehat h_{\hat\eta_u}(\hat\eta_u) - h_\eta(\eta)\big| \to 0 
  \quad\text{as}\quad 
   u \to \infty
\]
almost surely with respect to the densities * and *.
\end{lemma} 
\begin{proof} The existence of $(r_u)$ ensuring the required
convergence follows from Lemma~A.1 in \cite{C01}.  The rest of the
proof consists in the direct verification of the hypotheses (i)-(iv). 


The consistency of $\hat\eta_u$ follows by the EM argument exposed in
the main text because $(Z^{(r)}$, $\theta^{(r)}$, $\psi^{(r)})$ is 
asymptotically distributed according to $\pi(Z, \theta, \psi|M,
\eta)$. The second assertion follows by the continuity of the full 
conditional densities for $\theta$ with respect to $\eta'$
and the continuity of the hyperprior density for  $\psi$ with respect
to $\eta$. Likewise, the equicontinuity in (iii) follows 
by observing that $\widehat h_\eta$ is function of the prior and the
hyperprior densities, and the later are continuous respectively with
respect to $\psi$ and $\eta$. The ergodicity of the
Metropolis-within-Gibbs sampler considered throughout follows from the
Harris recurrence of the Markov chain $(Z^{(r)}, \theta^{(r)},  \psi^{(r)})$, $r \geqslant 1$ , see Theorem
12 in \cite{RR}. Harris recurrence is established by observing that
\textcolor{red}{... finish this!}.
\end{proof}

Let $\mathcal B = \sigma(\Theta)$ be the Borel sigma-algebra generated  
by the open sets of $\Theta$ and let a.e. stand for `almost everywhere'.

\begin{theorem} Under the conditions in Lemma~\ref{lem:technical},
  the statistic defined in \textup{(3)} satisfies
\[
  %\lim_{R, u \to\infty} 
  \sup_{S \in  \mathcal B} 
     \big| 
         \hat\pi(S|M, \hat\eta) -  \pi(S|M, \eta)
     \big| \to 0 \quad \text{as}\quad R, u \to \infty.
\]
\end{theorem}
\begin{proof}
From Lemma~\ref{lem:technical} it follows that $\widehat h_{\hat\eta_u}$
converges a.e. to $h_\eta$ as $u, R \to \infty$, that is
\[
  \lim_{u, R \to \infty} \widehat\pi(\theta|M, \hat\eta) 
  =
  \pi(\theta|M,\eta)\qquad\text{a.e.}
\]
This together with Scheff\'es Lemma, see for instance Theorem 7 in
\cite{DG}, implies that 
\[
  \lim_{u, R \to \infty} 
  \int \big|\widehat\pi(\theta|M, \hat\eta) - \pi(\theta|M,
  \eta)\big|\ \text{d}m(\theta) 
  =  
  0,
\]
where integration is with respect to Lebesgue measure defined in
$(\Theta, \mathcal B)$. This immediately gives
\begin{align*}
  \int \big|\widehat\pi(\theta &| M, \hat\eta) 
   - 
  \pi(\theta|M, \eta)\big|\ \text{d}m(\theta)                \\
  &=
  2 \sup_{B \in \mathcal B}\Bigg|
      \int_B \widehat\pi(B|M, \hat\eta)\ \text{d}m(\theta) 
      -
      \int_B \pi(B|M, \eta)\ \text{d}m(\theta)
   \Big| \to 0
\end{align*}
as $u, R \to \infty$, see Theorem 1 in \cite{DG}, and concludes
therefore the proof. 
\end{proof}


\section{Installing and running \texttt{signeR}}
\subsection{Installing \texttt{signeR}}
\texttt{signeR} is available as an R package and it can be installed
from R's prompt by typing
\begin{lstlisting}[]
  install.packages('signeR', dependencies=TRUE)
\end{lstlisting}
\textcolor{red}{RD, RV: please write.}


\subsection{Running \texttt{signeR}}
Write: A step-by-step example about how \texttt{signeR} would be run
on a real data example, probably on the 21 breast cancer data.
Loading the package \verb+library(signeR)+ and then typing
\verb+eBayesNMF()+ with the following arguments. Explain the data
structure for the input matrices $M$, $W$.

\begin{lstlisting}[]
  # this is a comment
  Mut<-read.table('21_breast_cancers.mutations.txt',header=FALSE)
  Opp<-read.table('21_breast_cancers.opportunity.txt',header=FALSE)
  Outsig<-signeR(M=Mut, Opport=Opp)  
  Paths(Outsig$SignExp)
  SignPlot(Outsig$SignExp)
  SignBoxPlot(Outsig$SignExp)
  Classify(Outsig$SignExp, labels=c(rep(`GradeII',10),
     rep(`GradeIII',10), NA))
  DiffExp(Outsig$SignExp, labels=c(rep(`GradeII',10),
     rep(`GradeIII',11)))
\end{lstlisting}


\section{Additional Results}
This section presents some information to complement the results
described in the main text. 

\subsection{21 Breast cancer data set}
The plots in Figure~\ref{fig:bcancerDATA} present the mutation
counts matrix and the estimated mutation count matrix. The sample
PD4120a has been left out for visualisation purposes, because it
contains extremely large counts.

The exposure matrix obtained by analysis the 21 breast cancer data
set, for the NMF rank $N=5$, is presented in
Figure~\ref{fig:21BCExposures}.

\subsection{Simulation study}
This section presents some additional figures related to the analysis  
of the synthetic data set generated according to the description in
Section 4.2 in the main text. 

The latest version of Emu, see \cite{FICMV}, was obtained by following
the instructions in
\indent\url{http://www.sanger.ac.uk/science/tools/emu}  which
redirects to\\ \indent \url{https://github.com/andrej-fischer/EMu}\\
The analyses with the framework described in \cite{A} were made by
downloading Mathlab code from
\indent\url{http://www.mathworks.com/matlabcentral/fileexchange/38724}

Figure~\ref{fig:COSMIC} here presents the signatures 1, 2, 3 and 13 as
obtained from Sanger's catalogue at\\
\indent\url{http://cancer.sanger.ac.uk/cosmic/signatures}\\ We observe
that this data set can be rather challenging from an inferential
perspective because of the similarity of signatures $S_2$ and $S_4$.

The signatures estimated by \texttt{signeR} are shown in
Figure~\ref{fig:signeR}. These correspond to the output generated by
only one of the 100 analyses considered for the same synthetic
data. The actual run shown here for illustration purposes was randomly
chosen. Figure~\ref{fig:BICS} presents the boxplots for the BIC values
obtained for this analysis. The BIC values obtained for other runs are
consistent wit this (data not shown).


Figure~\ref{fig:EMu} presents the signatures estimated by EMu  
in one of the analyses where it correctly estimated the NMF model
rank.

Figure~\ref{fig:LBA}\footnote{\textcolor{red}{RD, RR: Substitute the
actual figure by the correct data!}} presents the signatures obtained
in one of the 100 analyses made by using the method by Alexandrov, see
\cite{A}. As mentioned in the main text, these analyses were made by
setting the NMF rank to its correct value.

%\bibliographystyle: natbib, achemnat, plainnat, abbrv, plain
\bibliographystyle{alpha}
\bibliography{suppl}


\vspace{1cm}

{\footnotesize
\begin{tabular}{ll}
 \centering
  \parbox[c][4cm][t]{8cm}{
   {\sc Rafael A. Rosales}\\
   {\it 
   Departamento de Computa\c{c}\~ao e Matem\'atica\\
   Universidade de S\~ao Paulo\\
   Av. Bandeirantes, 3900, Ribeir\~ao Preto\\
   S\~ao Paulo 14049-901, Brazil\\}
   E-mail: \verb~rrosales@usp.br~
  }
&
  \parbox[c][4cm][t]{6.6cm}{
   {\sc Rodrigo D. Drummond\\
       Renan Valieris}\\
    {\it 
    Laboratory of Bioinformatics and Computational\\
    Biology, A. C. Camargo Cancer Center\\ 
    S\~ao Paulo 01509-010, Brazil\\}
    E-mails: \parbox[t]{2.5cm}{%
      \verb~rddrummond@cipe.accamargo.org.br~\\ 
      \verb~rvalieris@cipe.accamargo.org.br~}
 }
\\[-2em]
  \parbox[c][4cm][t]{6.6cm}{
   {\sc Emmanuel Dias-Neto}\\
    {\it 
    Laboratory of Medical Genomics\\ 
    A. C. Camargo Cancer Center\\
    S\~ao Paulo 01509-010, Brazil\\}
    E-mail: \parbox[t]{2.5cm}{%
      \verb~emmanuel@cipe.accamargo.org.br~}
 }
&
  \parbox[c][4cm][t]{6.6cm}{
   {\sc Israel T. da Silva}\\
   {\it 
    Laboratory of Bioinformatics and Computational\\
    Biology, A. C. Camargo Cancer Center\\ 
    S\~ao Paulo 01509-010, Brazil\\}
    and\\
   {\it Laboratory of Molecular Immunology\\
    The Rockefeller University\\
    1230 York Avenue, New York, NY 10065\\}
    E-mail: \verb~itojal@cipe.accamargo.org.br~
 }
\end{tabular}
}

%-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

\begin{center}
\begin{figure}
 \begin{tabular}{c}
 \includegraphics[width=9cm]{sfigs/21_Breast_Cancer_3D_M} 
 \\
 \includegraphics[width=9cm]{sfigs/21_Breast_Cancer_3D_Mhat} 
 \end{tabular}
 \caption{The 21 breast cancer data set (top) and the corresponding 
   estimated counts matrix $M$ obtained by \texttt{signeR} for the NMF
   model rank $N=5$.
 }\label{fig:bcancerDATA}
\end{figure}
\end{center}


\begin{center}
\begin{figure}
  \includegraphics[width=16cm]
     {sfigs/Exposure_boxplot_simulated_21bc_com_Opp}
  \caption{Exposures estimated by \texttt{signeR} for the  21 breast
     cancer data set.}\label{fig:21BCExposures}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
  \includegraphics[width=16cm]{sfigs/Signature_standard_Cosmic_plot}
  \caption{The four signatures used to simulate synthetic data
    sets.}\label{fig:COSMIC}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\includegraphics[width=16cm]{sfigs/Signatures_plot_simulated_21bc_com_Opp}
\caption{Signatures estimated by \texttt{signeR} while analysing the
  synthetic data  formed by the four signatures in
  Figure~\ref{fig:COSMIC}.}\label{fig:signeR} 
\end{figure}
\end{center}

\begin{center}
\begin{figure}
  \includegraphics[width=16cm]{sfigs/Signature_EMu_plot}
  \caption{Four signatures estimated by EMu while
  analysing the synthetic data  formed by the four signatures in
  Figure~\ref{fig:COSMIC}.}\label{fig:EMu}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
  \includegraphics[width=16cm]{sfigs/Signature_EMu_plot}
  \caption{Four signatures estimated by LBA while
  analysing the synthetic data  formed by the four signatures in
  Figure~\ref{fig:COSMIC}. The NMF model rank in all analyses made
  with LBA was set to its correct value.}\label{fig:LBA}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
  \includegraphics[width=7cm]{sfigs/BICs_Simulated_21bc_with_Opportunity}
  \caption{BIC values obtained by one of the 100 analyses made with
    \texttt{signeR} for the synthetic data set.}\label{fig:BICS}
\end{figure}
\end{center}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: